"""
TaylorKAN-ViT: Ultra-Efficient Vision Transformer
==================================================
A compact vision transformer using Kolmogorov-Arnold Networks (KAN) with 
Taylor series approximations for efficient image classification.

Key Features:
- <100K parameters
- Taylor series-based KAN layers
- B-spline basis functions
- Single-head attention mechanism

Author:Kaniz Fatema
--------------------------------------------------------------------------------------------------------------------------------------
import math
import torch
import torch.nn as nn
import torch.nn.functional as F


class CompactTaylorSeries(nn.Module):
    """Taylor series approximation with learnable scale parameter."""
    
    def __init__(self, n_terms: int = 3):
        super().__init__()
        self.n_terms = n_terms
        factorials = [math.factorial(2 * n + 1) for n in range(n_terms)]
        self.register_buffer("factorials", torch.tensor(factorials, dtype=torch.float32))
        self.scale = nn.Parameter(torch.ones(1))
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x_scaled = x * self.scale
        approximation = torch.zeros_like(x_scaled)
        x_power = x_scaled.clone()
        
        for n in range(self.n_terms):
            sign = (-1) ** n
            factorial_value = self.factorials[n].to(x.device, dtype=x.dtype)
            approximation = approximation + sign * x_power / factorial_value
            if n < self.n_terms - 1:
                x_power = x_power * x_scaled * x_scaled
        return approximation


class UltraCompactKANLinear(nn.Module):
    """
    Kolmogorov-Arnold Network (KAN) linear layer using B-spline basis functions.
    
    Args:
        in_features: Input dimension
        out_features: Output dimension
        grid_size: Number of grid points for splines
        spline_order: Order of B-spline polynomials
        grid_range: Range for grid initialization
    """
    
    def __init__(
        self,
        in_features: int,
        out_features: int,
        grid_size: int = 3,
        spline_order: int = 2,
        grid_range: tuple = (-1, 1),
    ):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.grid_size = grid_size
        self.spline_order = spline_order
        self.grid_eps = 0.02
        
        # Initialize grid
        h = (grid_range[1] - grid_range[0]) / grid_size
        grid = (
            torch.arange(-spline_order, grid_size + spline_order + 1) * h 
            + grid_range[0]
        ).expand(in_features, -1).contiguous()
        self.register_buffer("grid", grid)
        
        # Learnable parameters
        self.base_weight = nn.Parameter(torch.Tensor(out_features, in_features))
        self.spline_weight = nn.Parameter(
            torch.Tensor(out_features, in_features, grid_size + spline_order)
        )
        self.base_activation = nn.SiLU()
        
        self.reset_parameters()
    
    def reset_parameters(self):
        nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * 0.5)
        with torch.no_grad():
            noise = (torch.rand(self.out_features, self.in_features, 
                               self.grid_size + self.spline_order) - 0.5) * 0.02
            grid_slice = self.grid.T[self.spline_order : self.spline_order + noise.size(2)]
            coeffs = self.curve2coeff(grid_slice, noise)
            self.spline_weight.data.copy_(coeffs * 0.5)
    
    def b_splines(self, x: torch.Tensor) -> torch.Tensor:
        """Compute B-spline basis functions."""
        assert x.dim() == 2 and x.size(1) == self.in_features
        grid = self.grid.unsqueeze(0)
        x = x.unsqueeze(-1)
        bases = ((x >= grid[:, :, :-1]) & (x < grid[:, :, 1:])).to(x.dtype)
        
        for k in range(1, self.spline_order + 1):
            denom1 = grid[:, :, k:-1] - grid[:, :, :-(k + 1)]
            denom2 = grid[:, :, (k + 1):] - grid[:, :, 1:-k]
            denom1 = torch.where(denom1 == 0, torch.ones_like(denom1), denom1)
            denom2 = torch.where(denom2 == 0, torch.ones_like(denom2), denom2)
            bases = ((x - grid[:, :, :-(k + 1)]) / denom1 * bases[:, :, :-1]
                    + (grid[:, :, (k + 1):] - x) / denom2 * bases[:, :, 1:])
        return bases.contiguous()
    
    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """Convert curves to spline coefficients using least squares."""
        A = self.b_splines(x).permute(1, 0, 2)
        B = y.permute(1, 0, 2)
        out_features, in_features, basis_functions = y.size(0), y.size(1), A.size(2)
        coeffs = torch.zeros((out_features, in_features, basis_functions),
                            device=x.device, dtype=x.dtype)
        
        for i in range(in_features):
            A_i, B_i = A[i], B[i]
            for j in range(out_features):
                solution = torch.linalg.lstsq(A_i, B_i[j].unsqueeze(1)).solution
                coeffs[j, i, :] = solution.squeeze(1)
        return coeffs.contiguous()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        original_shape = x.shape
        x = x.view(-1, self.in_features)
        
        # Clamp input to grid range
        grid_min, grid_max = self.grid.min().item(), self.grid.max().item()
        x = torch.clamp(x, min=grid_min + self.grid_eps, max=grid_max - self.grid_eps)
        
        # Compute base and spline outputs
        base_output = F.linear(self.base_activation(x), self.base_weight)
        spline_basis = self.b_splines(x).view(x.size(0), -1)
        spline_output = F.linear(spline_basis, self.spline_weight.view(self.out_features, -1))
        
        output = base_output + spline_output
        return output.view(*original_shape[:-1], self.out_features)


class CompactPatchEmbedding(nn.Module):
    """Convert image into patch embeddings."""
    
    def __init__(self, in_channels: int = 3, patch_size: int = 16, embedding_dim: int = 32):
        super().__init__()
        self.patch_size = patch_size
        self.patcher = nn.Conv2d(in_channels, embedding_dim, kernel_size=patch_size,
                                stride=patch_size, padding=0)
        self.flatten = nn.Flatten(start_dim=2, end_dim=3)
        
        nn.init.kaiming_normal_(self.patcher.weight, mode='fan_out', nonlinearity='relu')
        if self.patcher.bias is not None:
            nn.init.zeros_(self.patcher.bias)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x_patched = self.patcher(x)
        x_flattened = self.flatten(x_patched)
        return x_flattened.permute(0, 2, 1)


class CompactAttention(nn.Module):
    """Single-head self-attention mechanism."""
    
    def __init__(self, embedding_dim: int = 32, attn_dropout: float = 0.1):
        super().__init__()
        self.layer_norm = nn.LayerNorm(embedding_dim, eps=1e-6)
        self.multihead_attn = nn.MultiheadAttention(
            embed_dim=embedding_dim,
            num_heads=1,
            dropout=attn_dropout,
            batch_first=True
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x_norm = self.layer_norm(x)
        attn_output, _ = self.multihead_attn(x_norm, x_norm, x_norm, need_weights=False)
        return attn_output


class UltraCompactTaylorKANBlock(nn.Module):
    """TaylorKAN MLP block with residual connection."""
    
    def __init__(
        self,
        embedding_dim: int = 32,
        mlp_size: int = 64,
        dropout: float = 0.15,
        taylor_terms: int = 3,
    ):
        super().__init__()
        self.layer_norm = nn.LayerNorm(embedding_dim, eps=1e-6)
        self.taylor = CompactTaylorSeries(n_terms=taylor_terms)
        
        self.kan1 = UltraCompactKANLinear(
            in_features=embedding_dim,
            out_features=mlp_size,
            grid_size=3,
            spline_order=2,
        )
        self.activation = nn.GELU()
        self.dropout1 = nn.Dropout(p=dropout)
        
        self.kan2 = UltraCompactKANLinear(
            in_features=mlp_size,
            out_features=embedding_dim,
            grid_size=3,
            spline_order=2,
        )
        self.dropout2 = nn.Dropout(p=dropout)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x_norm = self.layer_norm(x)
        z = self.taylor(x_norm)
        z = self.kan1(z)
        z = self.activation(z)
        z = self.dropout1(z)
        z = self.kan2(z)
        z = self.dropout2(z)
        return z


class CompactTransformerBlock(nn.Module):
    """Transformer encoder block with TaylorKAN."""
    
    def __init__(
        self,
        embedding_dim: int = 32,
        mlp_size: int = 64,
        attn_dropout: float = 0.1,
        mlp_dropout: float = 0.15,
        taylor_terms: int = 3,
    ):
        super().__init__()
        self.msa_block = CompactAttention(embedding_dim=embedding_dim, attn_dropout=attn_dropout)
        self.taylor_kan_block = UltraCompactTaylorKANBlock(
            embedding_dim=embedding_dim,
            mlp_size=mlp_size,
            dropout=mlp_dropout,
            taylor_terms=taylor_terms,
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.msa_block(x) + x
        x = self.taylor_kan_block(x) + x
        return x


class TaylorKANViT(nn.Module):
    """
    TaylorKAN-ViT: Ultra-Efficient Vision Transformer with KAN layers.
    
    Args:
        img_size: Input image size (assumes square images)
        in_channels: Number of input channels
        patch_size: Size of image patches
        num_classes: Number of output classes
        embedding_dim: Dimension of patch embeddings
        num_transformer_layers: Number of transformer blocks
        mlp_size: Hidden dimension in MLP blocks
        attn_dropout: Dropout rate for attention
        mlp_dropout: Dropout rate for MLP
        embedding_dropout: Dropout rate for embeddings
        head_dropout: Dropout rate for classification head
        taylor_terms: Number of terms in Taylor series
    """
    
    def __init__(
        self,
        img_size: int = 224,
        in_channels: int = 3,
        patch_size: int = 16,
        num_classes: int = 1000,
        embedding_dim: int = 32,
        num_transformer_layers: int = 2,
        mlp_size: int = 64,
        attn_dropout: float = 0.1,
        mlp_dropout: float = 0.15,
        embedding_dropout: float = 0.1,
        head_dropout: float = 0.2,
        taylor_terms: int = 3,
    ):
        super().__init__()
        assert img_size % patch_size == 0, "Image size must be divisible by patch size"
        
        self.img_size = img_size
        self.patch_size = patch_size
        self.embedding_dim = embedding_dim
        self.num_patches = (img_size // patch_size) ** 2
        
        # Patch embedding
        self.patch_embedding = CompactPatchEmbedding(
            in_channels=in_channels,
            patch_size=patch_size,
            embedding_dim=embedding_dim
        )
        
        # Positional embedding
        self.position_embedding = nn.Parameter(
            torch.randn(1, self.num_patches, embedding_dim) * 0.01
        )
        self.embedding_dropout = nn.Dropout(p=embedding_dropout)
        
        # Transformer encoder
        self.transformer_encoder = nn.Sequential(*[
            CompactTransformerBlock(
                embedding_dim=embedding_dim,
                mlp_size=mlp_size,
                attn_dropout=attn_dropout,
                mlp_dropout=mlp_dropout,
                taylor_terms=taylor_terms,
            )
            for _ in range(num_transformer_layers)
        ])
        
        # Classification head
        self.repr_norm = nn.LayerNorm(embedding_dim, eps=1e-6)
        self.classifier = nn.Sequential(
            nn.Dropout(p=head_dropout),
            nn.Linear(embedding_dim, num_classes)
        )
        
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """Initialize model weights."""
        if isinstance(module, (nn.Linear, nn.Conv2d)):
            nn.init.trunc_normal_(module.weight, std=0.015)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.LayerNorm):
            nn.init.ones_(module.weight)
            nn.init.zeros_(module.bias)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass.
        
        Args:
            x: Input tensor of shape (batch_size, channels, height, width)
            
        Returns:
            logits: Output tensor of shape (batch_size, num_classes)
        """
        # Patch embedding
        x = self.patch_embedding(x)
        
        # Add positional embedding
        x = x + self.position_embedding
        x = self.embedding_dropout(x)
        
        # Transformer encoder
        x = self.transformer_encoder(x)
        
        # Global average pooling
        x = self.repr_norm(x)
        x = x.mean(dim=1)
        
        # Classification
        logits = self.classifier(x)
        return logits
    
    def count_parameters(self):
        """Count total and trainable parameters."""
        total = sum(p.numel() for p in self.parameters())
        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)
        return total, trainable


# Example usage
if __name__ == "__main__":
    # Create model
    model = TaylorKANViT(
        img_size=224,
        in_channels=3,
        patch_size=16,
        num_classes=1000,
        embedding_dim=32,
        num_transformer_layers=2,
        mlp_size=64,
    )
    
    # Count parameters
    total_params, trainable_params = model.count_parameters()
    print(f"Total parameters: {total_params:,} ({total_params/1000:.1f}K)")
    print(f"Trainable parameters: {trainable_params:,}")
    
    # Test forward pass
    x = torch.randn(1, 3, 224, 224)
    output = model(x)
    print(f"Input shape: {x.shape}")
    print(f"Output shape: {output.shape}")
