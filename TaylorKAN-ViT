"""
TaylorKAN-ViT: Ultra-Efficient Vision Transformer for Medical Image Classification
==================================================================================
A parameter-efficient Vision Transformer that replaces conventional MLP layers
with Taylor-series-approximated Kolmogorov-Arnold Network (KAN) modules.

Architecture: 88.9K parameters, 4.9G FLOPs
Author: Kaniz Fatema
Affiliation: Wilfrid Laurier University
Paper: TaylorKAN-ViT: Enabling Parameter-Efficient Vision Transformers 
       for Medical Image Classification.
==================================================================================
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F


class CompactTaylorSeries(nn.Module):
    """
    Learnable-scale truncated Taylor series approximation for nonlinear feature enrichment.
    
    Implements odd-order Taylor expansion: f(x) = sx - (sx)³/3! + (sx)⁵/5!
    where s is a learnable scale parameter (initialized to 1.0).
    
    Args:
        n_terms (int): Number of Taylor terms (default: 3 for up to 5th order)
    
    Reference: Section II.C.2.b in the paper
    """
    def __init__(self, n_terms: int = 3):
        super().__init__()
        self.n_terms = n_terms
        
        # Precompute factorials for odd-order terms: 1!, 3!, 5!, ...
        factorials = [math.factorial(2 * n + 1) for n in range(n_terms)]
        self.register_buffer("factorials", torch.tensor(factorials, dtype=torch.float32))
        
        # Learnable scale parameter (initialized to 1.0)
        self.scale = nn.Parameter(torch.ones(1))
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Apply Taylor approximation with learnable scaling.
        
        Args:
            x: Input tensor of any shape
        
        Returns:
            Taylor-approximated tensor of the same shape
        """
        x_scaled = x * self.scale
        approximation = torch.zeros_like(x_scaled)
        x_power = x_scaled.clone()
        
        # Compute truncated odd-order Taylor series
        for n in range(self.n_terms):
            sign = (-1) ** n
            factorial_value = self.factorials[n].to(x.device, dtype=x.dtype)
            approximation = approximation + sign * x_power / factorial_value
            if n < self.n_terms - 1:
                x_power = x_power * x_scaled * x_scaled  # Increment by x²
        
        return approximation


class UltraCompactKANLinear(nn.Module):
    """
    Ultra-compact Kolmogorov-Arnold Network (KAN) linear layer using B-spline basis functions.
    
    Implements learnable univariate edge functions parameterized as:
        ϕ(x) = γ(SiLU(x) + Σ eᵣBᵣ(x))
    where Bᵣ are B-spline basis functions and eᵣ are spline coefficients.
    
    Args:
        in_features (int): Input dimension
        out_features (int): Output dimension
        grid_size (int): Number of B-spline grid intervals (default: 3)
        spline_order (int): Order of B-spline polynomials (default: 2 for quadratic)
        grid_range (tuple): Range for grid initialization (default: (-1, 1))
    
    Reference: Section II.C.2.a in the paper
    """
    def __init__(self, in_features: int, out_features: int, grid_size: int = 3, 
                 spline_order: int = 2, grid_range: tuple = (-1, 1)):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.grid_size = grid_size
        self.spline_order = spline_order
        self.grid_eps = 0.02  # Epsilon for clamping to avoid boundary issues
        
        # Initialize uniform grid for B-spline knots
        h = (grid_range[1] - grid_range[0]) / grid_size
        grid = (torch.arange(-spline_order, grid_size + spline_order + 1) * h + grid_range[0]
               ).expand(in_features, -1).contiguous()
        self.register_buffer("grid", grid)
        
        # Learnable weights: base (for SiLU) and spline (for B-spline basis)
        self.base_weight = nn.Parameter(torch.Tensor(out_features, in_features))
        self.spline_weight = nn.Parameter(torch.Tensor(out_features, in_features, grid_size + spline_order))
        self.base_activation = nn.SiLU()
        
        self.reset_parameters()
    
    def reset_parameters(self):
        """Initialize base and spline weights."""
        # Kaiming initialization for base weights
        nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * 0.5)
        
        # Initialize spline weights using curve-to-coefficient conversion
        with torch.no_grad():
            noise = (torch.rand(self.out_features, self.in_features, self.grid_size + self.spline_order) - 0.5) * 0.02
            grid_slice = self.grid.T[self.spline_order : self.spline_order + noise.size(2)]
            coeffs = self.curve2coeff(grid_slice, noise)
            self.spline_weight.data.copy_(coeffs * 0.5)
    
    def b_splines(self, x: torch.Tensor) -> torch.Tensor:
        """
        Compute B-spline basis functions using Cox-de Boor recursion.
        
        Args:
            x: Input tensor [batch*N, in_features]
        
        Returns:
            B-spline basis values [batch*N, in_features, num_bases]
        
        Reference: Section II.C.3 (Equations 10-11) in the paper
        """
        grid = self.grid.unsqueeze(0)  # [1, in_features, grid_points]
        x = x.unsqueeze(-1)  # [batch*N, in_features, 1]
        
        # Initialize with 0-order B-splines (indicator functions)
        bases = ((x >= grid[:, :, :-1]) & (x < grid[:, :, 1:])).to(x.dtype)
        
        # Cox-de Boor recursion for higher-order B-splines
        for k in range(1, self.spline_order + 1):
            denom1 = grid[:, :, k:-1] - grid[:, :, :-(k + 1)]
            denom2 = grid[:, :, (k + 1):] - grid[:, :, 1:-k]
            
            # Avoid division by zero
            denom1 = torch.where(denom1 == 0, torch.ones_like(denom1), denom1)
            denom2 = torch.where(denom2 == 0, torch.ones_like(denom2), denom2)
            
            # Recursive formula
            bases = ((x - grid[:, :, :-(k + 1)]) / denom1 * bases[:, :, :-1]
                    + (grid[:, :, (k + 1):] - x) / denom2 * bases[:, :, 1:])
        
        return bases.contiguous()
    
    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """
        Convert curves to B-spline coefficients using least-squares fitting.
        
        Args:
            x: Grid points [in_features, grid_size]
            y: Target values [out_features, in_features, grid_size]
        
        Returns:
            B-spline coefficients [out_features, in_features, num_bases]
        """
        A = self.b_splines(x).permute(1, 0, 2)  # [in_features, grid_size, num_bases]
        B = y.permute(1, 0, 2)  # [in_features, out_features, grid_size]
        coeffs = torch.zeros((y.size(0), y.size(1), A.size(2)), device=x.device, dtype=x.dtype)
        
        # Solve least-squares for each input-output pair
        for i in range(y.size(1)):
            for j in range(y.size(0)):
                solution = torch.linalg.lstsq(A[i], B[i][j].unsqueeze(1)).solution
                coeffs[j, i, :] = solution.squeeze(1)
        
        return coeffs.contiguous()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass combining base activation and B-spline transformation.
        
        Args:
            x: Input tensor [..., in_features]
        
        Returns:
            Output tensor [..., out_features]
        
        Implements: y = W_base·SiLU(x) + W_spline·B-splines(x)
        """
        original_shape = x.shape
        x = x.view(-1, self.in_features)
        
        # Clamp inputs to valid grid range with epsilon margin
        x = torch.clamp(x, min=self.grid.min().item() + self.grid_eps, 
                       max=self.grid.max().item() - self.grid_eps)
        
        # Compute base output (traditional linear layer with SiLU)
        base_output = F.linear(self.base_activation(x), self.base_weight)
        
        # Compute spline output (B-spline basis transformation)
        spline_basis = self.b_splines(x).view(x.size(0), -1)
        spline_output = F.linear(spline_basis, self.spline_weight.view(self.out_features, -1))
        
        # Combine base and spline components
        return (base_output + spline_output).view(*original_shape[:-1], self.out_features)


class CompactPatchEmbedding(nn.Module):
    """
    Efficient convolutional patch embedding module.
    
    Converts input image into sequence of patch tokens using a single convolution
    with kernel size and stride equal to patch size.
    
    Args:
        in_channels (int): Number of input channels (default: 3 for RGB)
        patch_size (int): Size of square patches (default: 16)
        embedding_dim (int): Dimension of patch embeddings (default: 32)
    
    Output: [batch_size, num_patches, embedding_dim] where num_patches = (H/P)×(W/P)
    
    Reference: Section II.B in the paper
    """
    def __init__(self, in_channels: int = 3, patch_size: int = 16, embedding_dim: int = 32):
        super().__init__()
        self.patcher = nn.Conv2d(in_channels, embedding_dim, kernel_size=patch_size,
                                stride=patch_size, padding=0)
        
        # Kaiming initialization for convolutional weights
        nn.init.kaiming_normal_(self.patcher.weight, mode='fan_out', nonlinearity='relu')
        if self.patcher.bias is not None:
            nn.init.zeros_(self.patcher.bias)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Input image [B, C, H, W]
        
        Returns:
            Patch embeddings [B, N, D] where N=(H/P)×(W/P), D=embedding_dim
        """
        return self.patcher(x).flatten(2).permute(0, 2, 1)


class CompactAttention(nn.Module):
    """
    Single-head self-attention mechanism with pre-normalization.
    
    Implements scaled dot-product attention to capture global context across
    all patch tokens efficiently using only one attention head.
    
    Args:
        embedding_dim (int): Dimension of token embeddings (default: 32)
        attn_dropout (float): Dropout rate for attention weights (default: 0.1)
    
    Reference: Section II.C.1 in the paper
    """
    def __init__(self, embedding_dim: int = 32, attn_dropout: float = 0.1):
        super().__init__()
        self.layer_norm = nn.LayerNorm(embedding_dim, eps=1e-6)
        self.multihead_attn = nn.MultiheadAttention(
            embed_dim=embedding_dim,
            num_heads=1,  # Single-head for efficiency
            dropout=attn_dropout,
            batch_first=True
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Input tokens [B, N, D]
        
        Returns:
            Attention output [B, N, D]
        
        Implements: Attention(Q, K, V) = softmax(QK^T/√D)V with dropout
        """
        x_norm = self.layer_norm(x)
        attn_output, _ = self.multihead_attn(x_norm, x_norm, x_norm, need_weights=False)
        return attn_output


class UltraCompactTaylorKANBlock(nn.Module):
    """
    TaylorKAN MLP block combining Taylor series approximation with KAN layers.
    
    Architecture:
        LayerNorm → Taylor(3 terms) → KAN(32→64) → GELU → Dropout →
        KAN(64→32) → Dropout
    
    This block replaces the traditional MLP feed-forward network in transformers,
    using learnable univariate functions instead of fixed activations.
    
    Args:
        embedding_dim (int): Input/output dimension (default: 32)
        mlp_size (int): Hidden layer expansion dimension (default: 64)
        dropout (float): Dropout rate (default: 0.15)
        taylor_terms (int): Number of Taylor terms (default: 3)
    
    Reference: Section II.C.2 in the paper
    """
    def __init__(self, embedding_dim: int = 32, mlp_size: int = 64, 
                 dropout: float = 0.15, taylor_terms: int = 3):
        super().__init__()
        self.layer_norm = nn.LayerNorm(embedding_dim, eps=1e-6)
        self.taylor = CompactTaylorSeries(n_terms=taylor_terms)
        
        # Two KAN layers with expansion: 32→64→32
        self.kan1 = UltraCompactKANLinear(embedding_dim, mlp_size, grid_size=3, spline_order=2)
        self.activation = nn.GELU()
        self.dropout1 = nn.Dropout(p=dropout)
        
        self.kan2 = UltraCompactKANLinear(mlp_size, embedding_dim, grid_size=3, spline_order=2)
        self.dropout2 = nn.Dropout(p=dropout)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Input tokens [B, N, D]
        
        Returns:
            Transformed tokens [B, N, D]
        """
        # Pre-normalization
        x_norm = self.layer_norm(x)
        
        # Taylor approximation for feature enrichment
        z = self.taylor(x_norm)
        
        # First KAN layer with expansion
        z = self.kan1(z)
        z = self.activation(z)
        z = self.dropout1(z)
        
        # Second KAN layer to restore dimension
        z = self.kan2(z)
        return self.dropout2(z)


class CompactTransformerBlock(nn.Module):
    """
    Complete transformer encoder block with self-attention and TaylorKAN MLP.
    
    Architecture:
        Input → [Attention + Residual] → [TaylorKAN MLP + Residual] → Output
    
    Args:
        embedding_dim (int): Token embedding dimension (default: 32)
        mlp_size (int): MLP hidden dimension (default: 64)
        attn_dropout (float): Attention dropout rate (default: 0.1)
        mlp_dropout (float): MLP dropout rate (default: 0.15)
        taylor_terms (int): Number of Taylor terms (default: 3)
    
    Reference: Section II.C in the paper
    """
    def __init__(self, embedding_dim: int = 32, mlp_size: int = 64, 
                 attn_dropout: float = 0.1, mlp_dropout: float = 0.15, taylor_terms: int = 3):
        super().__init__()
        self.msa_block = CompactAttention(embedding_dim=embedding_dim, attn_dropout=attn_dropout)
        self.taylor_kan_block = UltraCompactTaylorKANBlock(
            embedding_dim=embedding_dim,
            mlp_size=mlp_size,
            dropout=mlp_dropout,
            taylor_terms=taylor_terms
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Input tokens [B, N, D]
        
        Returns:
            Output tokens [B, N, D]
        """
        # Multi-head self-attention with residual connection
        x = self.msa_block(x) + x
        
        # TaylorKAN MLP with residual connection
        x = self.taylor_kan_block(x) + x
        
        return x


class TaylorKANViT(nn.Module):
    """
    TaylorKAN-ViT: Ultra-Efficient Vision Transformer for Medical Image Classification
    
    A parameter-efficient ViT that replaces all MLP layers with Taylor-series-approximated
    KAN modules. Designed for resource-constrained medical imaging applications.
    
    Total Parameters: 88.9K
    FLOPs: 4.9G
    
    Args:
        img_size (int): Input image size (assumes square images, default: 224)
        in_channels (int): Number of input channels (default: 3)
        patch_size (int): Size of image patches (default: 16)
        num_classes (int): Number of output classes (default: 1000)
        embedding_dim (int): Dimension of patch embeddings (default: 32)
        num_transformer_layers (int): Number of transformer blocks (default: 2)
        mlp_size (int): Hidden dimension in TaylorKAN blocks (default: 64)
        attn_dropout (float): Dropout rate for attention (default: 0.1)
        mlp_dropout (float): Dropout rate for TaylorKAN blocks (default: 0.15)
        embedding_dropout (float): Dropout rate after patch embedding (default: 0.1)
        head_dropout (float): Dropout rate before classification (default: 0.2)
        taylor_terms (int): Number of Taylor series terms (default: 3)
    
    Architecture:
        Input (224×224×3) → Patch Embedding (196 patches of 16×16)
        → Position Embedding → Dropout
        → Transformer Encoder (L=2 layers)
            ├── Single-Head Self-Attention
            └── TaylorKAN MLP Block
        → LayerNorm → Global Average Pooling
        → Dropout → Linear Classifier
    
    Reference: Section II in the paper
    """
    def __init__(self, img_size: int = 224, in_channels: int = 3, patch_size: int = 16,
                 num_classes: int = 1000, embedding_dim: int = 32, num_transformer_layers: int = 2,
                 mlp_size: int = 64, attn_dropout: float = 0.1, mlp_dropout: float = 0.15,
                 embedding_dropout: float = 0.1, head_dropout: float = 0.2, taylor_terms: int = 3):
        super().__init__()
        assert img_size % patch_size == 0, "Image size must be divisible by patch size"
        
        # Calculate number of patches: N = (H/P) × (W/P)
        self.num_patches = (img_size // patch_size) ** 2  # 196 for 224×224 with 16×16 patches
        
        # 1. Patch Embedding Module
        self.patch_embedding = CompactPatchEmbedding(in_channels, patch_size, embedding_dim)
        
        # 2. Learnable positional embeddings (initialized with small random values)
        self.position_embedding = nn.Parameter(torch.randn(1, self.num_patches, embedding_dim) * 0.01)
        self.embedding_dropout = nn.Dropout(p=embedding_dropout)
        
        # 3. Transformer Encoder (L=2 identical layers)
        self.transformer_encoder = nn.Sequential(*[
            CompactTransformerBlock(embedding_dim, mlp_size, attn_dropout, mlp_dropout, taylor_terms)
            for _ in range(num_transformer_layers)
        ])
        
        # 4. Classification Head
        self.repr_norm = nn.LayerNorm(embedding_dim, eps=1e-6)
        self.classifier = nn.Sequential(
            nn.Dropout(p=head_dropout),
            nn.Linear(embedding_dim, num_classes)
        )
        
        # Initialize all weights
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """
        Initialize model weights using truncated normal and zeros.
        
        - Linear/Conv2d: Truncated normal (std=0.015)
        - LayerNorm: Ones for weight, zeros for bias
        """
        if isinstance(module, (nn.Linear, nn.Conv2d)):
            nn.init.trunc_normal_(module.weight, std=0.015)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.LayerNorm):
            nn.init.ones_(module.weight)
            nn.init.zeros_(module.bias)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through the TaylorKAN-ViT model.
        
        Args:
            x: Input images [B, C, H, W]
        
        Returns:
            Class logits [B, num_classes]
        
        Pipeline:
            1. Patch embedding: [B,C,H,W] → [B,N,D]
            2. Add positional embedding and apply dropout
            3. Process through L transformer layers
            4. Global average pooling: [B,N,D] → [B,D]
            5. Classification: [B,D] → [B,num_classes]
        """
        # Step 1: Convert image to patch embeddings
        x = self.patch_embedding(x)  # [B, 196, 32]
        
        # Step 2: Add learnable positional information
        x = x + self.position_embedding
        x = self.embedding_dropout(x)
        
        # Step 3: Process through transformer encoder blocks
        x = self.transformer_encoder(x)
        
        # Step 4: Prepare for classification
        x = self.repr_norm(x)
        x = x.mean(dim=1)  # Global average pooling across all tokens
        
        # Step 5: Classification
        return self.classifier(x)
