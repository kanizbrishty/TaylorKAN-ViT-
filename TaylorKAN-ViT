"""
TaylorKAN-ViT: Ultra-Efficient Vision Transformer for Medical Image Classification
Author: Kaniz Fatema
Affiliation: Wilfrid Laurier University
Paper: IEEE EMBC 2025

Architecture: 88.9K parameters | 4.9G FLOPs
Performance: 94.36% (PneumoniaMNIST) | 95.90% (CPNX-ray) | 61.00% (PAD-UFES-20) | 70.50% (Kvasir)
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F


class CompactTaylorSeries(nn.Module):
    """Learnable-scale truncated Taylor series: f(x) = sx - (sx)³/3! + (sx)⁵/5!"""
    
    def __init__(self, n_terms: int = 3):
        super().__init__()
        self.n_terms = n_terms
        # Precompute factorials for odd-order terms: 1!, 3!, 5!
        factorials = [math.factorial(2 * n + 1) for n in range(n_terms)]
        self.register_buffer("factorials", torch.tensor(factorials, dtype=torch.float32))
        self.scale = nn.Parameter(torch.ones(1))  # Learnable scale parameter
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x_scaled = x * self.scale
        approximation = torch.zeros_like(x_scaled)
        x_power = x_scaled.clone()
        
        # Compute truncated odd-order Taylor series
        for n in range(self.n_terms):
            sign = (-1) ** n
            factorial_value = self.factorials[n].to(x.device, dtype=x.dtype)
            approximation = approximation + sign * x_power / factorial_value
            if n < self.n_terms - 1:
                x_power = x_power * x_scaled * x_scaled
        return approximation


class UltraCompactKANLinear(nn.Module):
    """KAN layer with B-spline basis: y = W_base·SiLU(x) + W_spline·B-splines(x)"""
    
    def __init__(self, in_features: int, out_features: int, grid_size: int = 3, 
                 spline_order: int = 2, grid_range: tuple = (-1, 1)):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.grid_size = grid_size
        self.spline_order = spline_order
        self.grid_eps = 0.02
        
        # Initialize B-spline knot grid
        h = (grid_range[1] - grid_range[0]) / grid_size
        grid = (torch.arange(-spline_order, grid_size + spline_order + 1) * h + grid_range[0]
               ).expand(in_features, -1).contiguous()
        self.register_buffer("grid", grid)
        
        # Learnable weights for base and spline components
        self.base_weight = nn.Parameter(torch.Tensor(out_features, in_features))
        self.spline_weight = nn.Parameter(torch.Tensor(out_features, in_features, grid_size + spline_order))
        self.base_activation = nn.SiLU()
        self.reset_parameters()
    
    def reset_parameters(self):
        """Initialize weights using Kaiming uniform and curve fitting"""
        nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * 0.5)
        with torch.no_grad():
            noise = (torch.rand(self.out_features, self.in_features, self.grid_size + self.spline_order) - 0.5) * 0.02
            grid_slice = self.grid.T[self.spline_order : self.spline_order + noise.size(2)]
            coeffs = self.curve2coeff(grid_slice, noise)
            self.spline_weight.data.copy_(coeffs * 0.5)
    
    def b_splines(self, x: torch.Tensor) -> torch.Tensor:
        """Compute B-spline basis functions using Cox-de Boor recursion"""
        grid = self.grid.unsqueeze(0)
        x = x.unsqueeze(-1)
        bases = ((x >= grid[:, :, :-1]) & (x < grid[:, :, 1:])).to(x.dtype)
        
        # Cox-de Boor recursion for higher-order splines
        for k in range(1, self.spline_order + 1):
            denom1 = grid[:, :, k:-1] - grid[:, :, :-(k + 1)]
            denom2 = grid[:, :, (k + 1):] - grid[:, :, 1:-k]
            denom1 = torch.where(denom1 == 0, torch.ones_like(denom1), denom1)
            denom2 = torch.where(denom2 == 0, torch.ones_like(denom2), denom2)
            bases = ((x - grid[:, :, :-(k + 1)]) / denom1 * bases[:, :, :-1]
                    + (grid[:, :, (k + 1):] - x) / denom2 * bases[:, :, 1:])
        return bases.contiguous()
    
    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """Convert curves to B-spline coefficients via least-squares"""
        A = self.b_splines(x).permute(1, 0, 2)
        B = y.permute(1, 0, 2)
        coeffs = torch.zeros((y.size(0), y.size(1), A.size(2)), device=x.device, dtype=x.dtype)
        
        for i in range(y.size(1)):
            for j in range(y.size(0)):
                solution = torch.linalg.lstsq(A[i], B[i][j].unsqueeze(1)).solution
                coeffs[j, i, :] = solution.squeeze(1)
        return coeffs.contiguous()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        original_shape = x.shape
        x = x.view(-1, self.in_features)
        # Clamp to valid grid range
        x = torch.clamp(x, min=self.grid.min().item() + self.grid_eps, 
                       max=self.grid.max().item() - self.grid_eps)
        
        # Combine base and spline outputs
        base_output = F.linear(self.base_activation(x), self.base_weight)
        spline_basis = self.b_splines(x).view(x.size(0), -1)
        spline_output = F.linear(spline_basis, self.spline_weight.view(self.out_features, -1))
        
        return (base_output + spline_output).view(*original_shape[:-1], self.out_features)


class CompactPatchEmbedding(nn.Module):
    """Convolutional patch embedding: [B,C,H,W] → [B,N,D]"""
    
    def __init__(self, in_channels: int = 3, patch_size: int = 16, embedding_dim: int = 32):
        super().__init__()
        self.patcher = nn.Conv2d(in_channels, embedding_dim, kernel_size=patch_size,
                                stride=patch_size, padding=0)
        nn.init.kaiming_normal_(self.patcher.weight, mode='fan_out', nonlinearity='relu')
        if self.patcher.bias is not None:
            nn.init.zeros_(self.patcher.bias)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.patcher(x).flatten(2).permute(0, 2, 1)


class CompactAttention(nn.Module):
    """Single-head self-attention with pre-normalization"""
    
    def __init__(self, embedding_dim: int = 32, attn_dropout: float = 0.1):
        super().__init__()
        self.layer_norm = nn.LayerNorm(embedding_dim, eps=1e-6)
        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=1,
                                                     dropout=attn_dropout, batch_first=True)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x_norm = self.layer_norm(x)
        attn_output, _ = self.multihead_attn(x_norm, x_norm, x_norm, need_weights=False)
        return attn_output


class UltraCompactTaylorKANBlock(nn.Module):
    """TaylorKAN MLP: LayerNorm → Taylor → KAN(32→64) → GELU → KAN(64→32)"""
    
    def __init__(self, embedding_dim: int = 32, mlp_size: int = 64, 
                 dropout: float = 0.15, taylor_terms: int = 3):
        super().__init__()
        self.layer_norm = nn.LayerNorm(embedding_dim, eps=1e-6)
        self.taylor = CompactTaylorSeries(n_terms=taylor_terms)
        self.kan1 = UltraCompactKANLinear(embedding_dim, mlp_size, grid_size=3, spline_order=2)
        self.activation = nn.GELU()
        self.dropout1 = nn.Dropout(p=dropout)
        self.kan2 = UltraCompactKANLinear(mlp_size, embedding_dim, grid_size=3, spline_order=2)
        self.dropout2 = nn.Dropout(p=dropout)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x_norm = self.layer_norm(x)
        z = self.taylor(x_norm)
        z = self.kan1(z)
        z = self.activation(z)
        z = self.dropout1(z)
        z = self.kan2(z)
        return self.dropout2(z)


class CompactTransformerBlock(nn.Module):
    """Transformer block: [Attention + Residual] → [TaylorKAN MLP + Residual]"""
    
    def __init__(self, embedding_dim: int = 32, mlp_size: int = 64, 
                 attn_dropout: float = 0.1, mlp_dropout: float = 0.15, taylor_terms: int = 3):
        super().__init__()
        self.msa_block = CompactAttention(embedding_dim=embedding_dim, attn_dropout=attn_dropout)
        self.taylor_kan_block = UltraCompactTaylorKANBlock(embedding_dim=embedding_dim, mlp_size=mlp_size,
                                                            dropout=mlp_dropout, taylor_terms=taylor_terms)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.msa_block(x) + x  # Attention with residual
        x = self.taylor_kan_block(x) + x  # TaylorKAN MLP with residual
        return x


class TaylorKANViT(nn.Module):
    """
    TaylorKAN-ViT: Ultra-Efficient Vision Transformer (88.9K params, 4.9G FLOPs)
    
    Architecture:
        Input (224×224×3) → Patch Embedding (196 tokens, dim=32)
        → Position Embedding → Transformer Encoder (L=2)
        → Global Avg Pool → Classifier
    
    Args:
        img_size: Input image size (default: 224)
        in_channels: Number of input channels (default: 3)
        patch_size: Patch dimension (default: 16)
        num_classes: Number of classes (default: 1000)
        embedding_dim: Token dimension (default: 32)
        num_transformer_layers: Encoder depth (default: 2)
        mlp_size: Hidden dimension (default: 64)
        attn_dropout: Attention dropout (default: 0.1)
        mlp_dropout: MLP dropout (default: 0.15)
        embedding_dropout: Embedding dropout (default: 0.1)
        head_dropout: Classifier dropout (default: 0.2)
        taylor_terms: Taylor series terms (default: 3)
    """
    
    def __init__(self, img_size: int = 224, in_channels: int = 3, patch_size: int = 16,
                 num_classes: int = 1000, embedding_dim: int = 32, num_transformer_layers: int = 2,
                 mlp_size: int = 64, attn_dropout: float = 0.1, mlp_dropout: float = 0.15,
                 embedding_dropout: float = 0.1, head_dropout: float = 0.2, taylor_terms: int = 3):
        super().__init__()
        assert img_size % patch_size == 0, "Image size must be divisible by patch size"
        
        self.num_patches = (img_size // patch_size) ** 2  # 196 for 224×224 with 16×16 patches
        self.patch_embedding = CompactPatchEmbedding(in_channels, patch_size, embedding_dim)
        self.position_embedding = nn.Parameter(torch.randn(1, self.num_patches, embedding_dim) * 0.01)
        self.embedding_dropout = nn.Dropout(p=embedding_dropout)
        
        # Transformer encoder with L=2 identical blocks
        self.transformer_encoder = nn.Sequential(*[
            CompactTransformerBlock(embedding_dim, mlp_size, attn_dropout, mlp_dropout, taylor_terms)
            for _ in range(num_transformer_layers)
        ])
        
        # Classification head
        self.repr_norm = nn.LayerNorm(embedding_dim, eps=1e-6)
        self.classifier = nn.Sequential(nn.Dropout(p=head_dropout), nn.Linear(embedding_dim, num_classes))
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """Initialize weights: truncated normal for Linear/Conv2d, ones/zeros for LayerNorm"""
        if isinstance(module, (nn.Linear, nn.Conv2d)):
            nn.init.trunc_normal_(module.weight, std=0.015)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.LayerNorm):
            nn.init.ones_(module.weight)
            nn.init.zeros_(module.bias)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass: Image → Patches → Transformer → Classification
        
        Args:
            x: Input images [B, C, H, W]
        
        Returns:
            Class logits [B, num_classes]
        """
        x = self.patch_embedding(x)  # [B, 196, 32]
        x = x + self.position_embedding
        x = self.embedding_dropout(x)
        x = self.transformer_encoder(x)
        x = self.repr_norm(x)
        x = x.mean(dim=1)  # Global average pooling
        return self.classifier(x)
